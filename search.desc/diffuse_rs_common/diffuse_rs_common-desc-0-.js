searchState.loadedDescShard("diffuse_rs_common", 0, "Source from which to load the model. This is easiest to …\nNice progress bar with over an iterator and a message. …\nThe source of the HF token.\nA simple <code>VarBuilder</code>, this is less generic than …\nLoad a DDUF model from a .dduf file.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInitializes a <code>VarBuilder</code> using a custom backend.\nInitializes a <code>VarBuilder</code> from a binary buffer in the …\nLoad tensors into a VarBuilder backed by a VarMap using …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nLoad the model from a Hugging Face model ID or a local …\nInitializes a <code>VarBuilder</code> from a binary slice in the …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nThis reads a token from a specified source. If the token …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\ncandle-nn\nLoad the transformer part of this model from a Hugging …\nRead a file.\nRead a file, always returning owned data.\nInitializes a <code>VarBuilder</code> that uses zeros for any tensor.\nUnary ops that can be defined in user-land.\nThe different types of elements allowed in tensors.\nA <code>DeviceLocation</code> represents a physical device whereas …\nTrait used to implement multiple signatures for ease of …\nIndexing via a 1d tensor\nUnary ops that can be defined in user-land. These ops work …\nThis is a regular slice, purely indexing a chunk of the …\nThis selects the elements for which an index has some …\nAn iterator over offset position for items of an …\nThe core struct for manipulating tensors.\nUnique identifier for tensors.\nGeneric structure used to index a slice of the tensor\nA variable is a wrapper around a tensor, however variables …\nThis operation multiplies the input tensor by <code>mul</code> then …\nRun the <code>forward</code> method of <code>m</code> on <code>self</code>.\nApplies a unary custom op.\nApplies a unary custom op without backward support\nApplies a binary custom op.\nApplies a binary custom op without backward support\nApplies a ternary custom op.\nApplies a ternary custom op without backward support\nRun the <code>forward</code> method of <code>m</code> on <code>self</code>.\nCreates a new 1D tensor with values from the interval …\nCreates a new 1D tensor with values from the interval …\nReturns the indices that sort the tensor along the last …\nSimilar to <code>argmax_keepdim</code> but the target dimension is …\nSimilar to <code>argmin_keepdim</code> but the target dimension is …\nString representation for dtypes.\n2D average pooling over an input tensor with multiple …\nSame as <code>avg_pool2d</code> but with a <code>stride</code> that can be set to a …\nReturn <code>BF16</code> for devices that support it, otherwise default …\nBroadcast the input tensor to the target shape. This …\nReturns a new tensor duplicating data from the original …\nMatrix-multiplication with broadcasting support.\nMatrix-multiplication with broadcasting support and fused …\nBroadcasting version of <code>pow</code>.\nThis function takes as argument the argument <code>arg</code> used in …\nThis function takes as argument the argument <code>arg</code> used in …\nConcatenates two or more tensors along a particular …\nSplit a tensor into the specified number of chunks, this …\nClamp the tensor values to be between <code>min</code> and <code>max</code>.\nElement-wise comparison between two tensors, e.g. …\nReturns a tensor that is in row major order. This is the …\nApplies a 1D convolution over the input tensor.\nApplies a 2D convolution over the input tensor.\nApplies a 1D transposed convolution over the input tensor.\nApplies a 2D transposed convolution over the input tensor.\nCompared to clone, this copies the actual storage but may …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nReturns the cumulative sum of elements of the input tensor …\nReturns a new tensor detached from the current graph, …\nThe device on which the input tensor is located.\nThe dimension size for a specified dimension index.\nThe dimension size for this tensor on each axis.\nThe dtype for the elements stored in the input tensor.\nThe number of elements stored in this tensor.\nApplies the Exponential Linear Unit (ELU) function on each …\nReturns a tensor with the values from the <code>self</code> tensor at …\nCreates a new tensor filled with uninitialized memory.\nCreates a new tensor filled with uninitialized memory of …\nElement-wise equality.\nAn alias for broadcast_as.\nReturns a matrix with a diagonal of ones of size n by n.\nFlattens the input tensor on the dimension indexes from …\nFlattens the input tensor by reshaping it into a one …\nFlattens the input tensor on the dimension indexes from …\nFlattens the input tensor on the dimension indexes from <code>0</code> …\nReturns a tensor that is in row major order. This always …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreates a new 1D tensor from an iterator.\nCreates a new tensor initialized with values from the …\nCreates a fresh tensor structure based on a storage and a …\nCreates a new tensor initialized with values from the …\nReturns a new tensor with all the elements having the same …\nGather values across the target dimension.\nElement-wise comparison with greater-equal, the returned …\nReturns the sub-tensor fixing the index at <code>i</code> on the first …\nGet the current seed for the device RNG.\nReturns the sub-tensor fixing the index at <code>index</code> on the …\nElement-wise comparison with greater-than, the returned …\nReturns a slicing iterator which are the chunks of data …\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nThe unique identifier for this tensor.\nAccumulate element from <code>source</code> at indexes <code>indexes</code> and add …\nSelect values for the input tensor at the target indexes …\nApplies a unary custom op in place.\nApplies a unary custom op in place (for the first tensor).\nApplies a ternary custom op in place (for the first …\nInterpolate the input tensor to the <code>target_size</code> size, …\nInterpolate the input tensor to the <code>(target_h, target_w)</code> …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConsumes this <code>Var</code> and return the underlying tensor.\nReturns true if the data is stored in a C contiguous (aka …\nReturns true if the data is stored in a Fortran contiguous …\nWhether this tensor is a variable or not. A variable is a …\nThe layout of the input tensor, this stores both the shape …\nElement-wise comparison with lower-equal, the returned …\nReturns log(sum(exp(tensor), dim)).\nElement-wise comparison with lower-than, the returned …\nReturns the matrix-multiplication of the input tensor with …\nReturns the matrix-multiplication of the input tensor with …\nReturns the matrix-multiplication of the input tensor with …\nSimilar to <code>max_keepdim</code> but the target dimension is …\nGathers the maximum value across the selected dimension. …\n2D max pooling over an input tensor with multiple channels.\nSame as <code>max_pool2d</code> but with a <code>stride</code> that can be set to a …\nReturns the mean of all elements in the input tensor. The …\nReturns the mean of all elements in the input tensor. The …\nCreates grids of coordinates specified by the 1D inputs.\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nSimilar to <code>min_keepdim</code> but the target dimension is …\nGathers the minimum value across the selected dimension. …\nReturns a new tensor that is a narrowed version of the …\nElement-wise non-equality.\nCreates a new tensor on the specified device using the …\nCreates a new tensor on the specified device using the …\nNormalize a ‘relative’ axis value: positive values are …\nNumpy support for tensors.\nCreates a new tensor filled with ones.\nCreates a new tensor filled with ones with same shape, …\nPad the input tensor using same values along dimension <code>dim</code>…\nPad the input tensor using 0s along dimension <code>dim</code>. This …\nReturns a tensor with the same data as the input where the …\nPointwise pow operation.\nRaise the tensor to some float exponent <code>e</code>.\nCreates a new tensor initialized with values sampled …\nCreates a new tensor initialized with values sampled from …\nThe number of dimensions for this tensor, 0 for a scalar …\nReads a npy file and return the stored multi-dimensional …\nReads a npz file and returns the stored multi-dimensional …\nReads a npz file and returns the stored multi-dimensional …\nRepeat this tensor along the specified dimensions.\nReshape returns a tensor with the target shape provided …\nRoll the tensor input along the given dimension. Elements …\nRound element of the input tensor to the nearest integer.\nSets the content of the inner tensor, this does not …\nThe shape of a tensor is a tuple with the size of each of …\nThe tensor shape, i.e. dimension sizes on each axis.\nThe size used by each element in bytes, i.e. 1 for <code>U8</code>, 4 …\nReturns a copy of <code>self</code> where the values within <code>ranges</code> have …\nEmbeds the values of the <code>src</code> tensor into the <code>self</code> tensor …\nEmbeds the values of the <code>src</code> tensor into the <code>self</code> tensor …\nSet the values on <code>self</code> using values from <code>src</code>. The copy …\nSorts the tensor along the last dimension, returns the …\nCreates a new tensor with the specified dimension removed …\nStacks two or more tensors along a particular dimension.\nThe storage used by this tensor, together with the layout …\nSimilar to <code>strided_index</code> but returns the position of the …\nReturns an iterator over position of the elements in the …\nReturns the sum of all elements in the input tensor. The …\nComputes the sum of all the elements in this tensor and …\nReturns the sum of all elements in the input tensor. The …\nReturns a tensor that is a transposed version of the …\nIf the target device is the same as the tensor device, …\nCasts the input tensor to the target <code>dtype</code>.\nRetrieves the single scalar value hold in the tensor. If …\nAn alias for <code>to_scalar</code>.\nReturns the data contained in a 1D tensor as a vector of …\nReturns the data contained in a 2D tensor as a vector of …\nReturns the data contained in a 3D tensor.\nReturns true if the computation graph should track this …\nReturns a tensor that is a transposed version of the …\nReturns a lower triangular matrix of ones of size n by n.\nReturns an upper triangular matrix of ones of size n by n.\nReturns a view of which contains all slices of size <code>size</code> …\nCreates a new tensor with a dimension of size one inserted …\nAlias for <code>interpolate1d</code>.\nAlias for <code>interpolate2d</code>.\nReturns the unbiased variance over the selected dimension.\nReturns the unbiased variance over the selected dimension.\nReturns a tensor with the same shape as the input tensor, …\nWrites a multi-dimensional array in the npy format.\nWrites multiple multi-dimensional arrays using the npz …\nCreates a new tensor filled with zeros.\nCreates a new tensor filled with ones with same shape, …\nSafety\nSynchronize should block until all the operations on the …\nA store for gradients, associating a tensor id to the …\nReturns the argument unchanged.\nGet the gradient tensor associated with the given tensor\nGet the gradient tensor corresponding to the given tensor …\nGet the tensor ids of the stored gradient tensors\nInsert a gradient tensor associated with the given tensor, …\nCalls <code>U::from(self)</code>.\nRemove the gradient tensor associated with the given …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nProvides the error and related functions\n<code>erf</code> calculates the error function at <code>x</code>.\n<code>erf_inv</code> calculates the inverse error function at <code>x</code>.\n<code>erfc</code> calculates the complementary error function at <code>x</code>.\n<code>erfc_inv</code> calculates the complementary inverse error …\nDot-product of two vectors.\nMaximum element in a non-empty vector.\nMinimum element in a non-empty vector.\nSum of all elements in a vector.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nOptions for Tensor pretty printing\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nThis bool controls whether reduced precision reductions …\nThis bool controls whether reduced precision reductions …\nThis bool controls whether reduced precision reductions …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThis bool controls whether reduced precision reductions …\nThis bool controls whether reduced precision reductions …\nThis bool controls whether reduced precision reductions …\nAttach more context to an error.\nContains the error value\nMain library error type.\nUtf8 parse error.\nI/O error.\nUser generated error message, typically created via <code>bail!</code>.\nContains the success value\nInteger parse error.\nSafeTensor error.\nAdding path information to an error.\nArbitrary errors wrapping.\nArbitrary errors wrapping with context.\nZip file format error.\nWrap the error value with additional context.\nCreate a new error based on a debuggable error message.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new error based on a printable error message.\nWrap the error value with additional context that is …\nCreate a new error by wrapping another.\nReturns the appropriate start and stop offset if the data …\nThe dimension size for a specified dimension index.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if the data is stored in a C contiguous (aka …\nReturns true if the data is stored in a Fortran contiguous …\nLazy tensor loader.\nReturns the argument unchanged.\nThis only returns the shape and dtype for a named tensor. …\nCalls <code>U::from(self)</code>.\n<code>BackpropOp</code> is a wrapper around <code>Option&lt;Op&gt;</code>. The main goal …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLazy tensor loader.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nRead all the tensors from a PyTorch pth file.\nRead all the tensors from a PyTorch pth file with a given …\nRead the tensor info from a .pth file.\nThe block size, i.e. the number of elements stored in each …\nThe block dtype\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nSupport for the GGML file format.\nSupport for the GGUF file format.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nQuantize <code>src</code> (currently on the CPU) to a QTensor on <code>dev</code>\nQuantize <code>src</code> (currently on the CPU) to a QTensor on <code>dev</code>\nThe type size for blocks in bytes.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a [Tensor] from a raw GGML tensor.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThis will also automatically upcast any integral types …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nDot product used as a building block for quantized mat-mul.\nGeneric implementation of the dot product without simd …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a wrapper around multiple memory mapped file and …\nCreates a wrapper around a memory mapped file and …\nCreates a wrapper around a binary buffer and deserialize …\nCreates a wrapper around a binary buffer and deserialize …\nCreates a wrapper around a memory mapped file from which …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck whether the two shapes are compatible for broadcast, …\nThe dimension size for a specified dimension index.\nThe dimensions as a slice of <code>usize</code>.\nThe total number of elements, this is the product of all …\nModifies the shape by adding a list of additional …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if the strides are C contiguous (aka row …\nReturns true if the strides are Fortran contiguous (aka …\nThe rank is the number of dimensions, 0 for a scalar …\nSimple wrapper that doesn’t do any buffering.\nA stream tensor is used in streaming module. It can either …\nStreaming modules take as input a stream tensor and return …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSplits the Streaming Tensor on the time axis <code>dim</code> with the …\nActivation Functions\nBatch Normalization.\nConvolution Layers.\nEmbedding Layer.\nEncoding Utilities. (e.g., one-hot/cold encoding)\nLayers defined by closures.\nGroup Normalization.\nVariable initialization.\nCache Implementations\nLayer Normalization.\nLinear layer\nLoss Calculations\nTensor ops.\nVarious optimization algorithms.\nRecurrent Neural Networks\nRotary Embeddings\nSequential Layer\nA <code>VarBuilder</code> is used to retrieve variables used by a …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate or initialize a new PReLU layer.\nComputes (softmax(QK^T*sqrt(d_k)) + M)V. <code>M</code> is the …\nThe meaning of affine here is different from LayerNorm: …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nControls exponential moving average of running stats. …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nGet the hidden size of the embedding matrix\nCalls <code>U::from(self)</code>.\nOne-hot/cold encoding.\nA layer defined by a simple closure.\nA layer defined by a simple closure.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nConstant value.\nNumber of features as input or output of a layer. In …\nVariable initializations.\nKaiming uniform initialization. See “Delving deep into …\nThe non-linear function that follows this layer. ReLU is …\nRandom normal with some mean and standard deviation.\nUniform initialization between some lower and upper bounds.\nCompute the fan-in or fan-out value for a weight tensor of …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a new tensor with the specified shape, device, and …\nReturns the attn_mask to be applied <em>after</em> adding <code>seq_len</code> …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nRmsNorm is a specialized version of the LayerNorm module.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nWhether to remove the mean or not, the default is true and …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreate or initialize a new linear layer.\nCreate or initialize a new linear layer without biases.\nThe binary cross-entropy with logit loss.\nThe cross-entropy loss.\nThe mean squared error loss.\nThe negative log likelihood loss.\nSoftmax with fused broadcast addition of a mask and scale. …\nReturns the argument unchanged.\nReturns the argument unchanged.\nInplace equivalent of <code>attn_softmax_last_dim</code>\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nScaled dot product attention with a fused kernel.\nApplies the softmax function to the input tensor, …\nThe interface optimizers should implement.\nOptimizer for Stochastic Gradient Descent.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nA Gated Recurrent Unit (GRU) layer.\nThe state for a GRU network, this contains a single tensor.\nA Long Short-Term Memory (LSTM) layer.\nThe state for a LSTM network, this contains two tensors.\nTrait for Recurrent Neural Networks.\nThe cell state vector.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nThe hidden state vector, which is also the output of the …\nThe hidden state vector, which is also the output of the …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a LSTM layer.\nCreates a LSTM layer.\nCreates a GRU layer.\nApplies multiple steps of the recurrent network.\nApplies multiple steps of the recurrent network.\nConverts a sequence of state to a tensor.\nApplies a single step of the recurrent network.\nA zero state from which the recurrent network is usually …\nThis may modify the tensors in place!\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nA sequential layer combining multiple other layers.\nAppends a layer after all the current layers.\nAppends a closure after all the current layers.\nApplies the forward pass and returns the output for each …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns true if this layer does not have any sub-layer.\nThe number of sub-layers embedded in this layer.\nCreates a new empty sequential layer.\nA trait that defines how tensor data is retrieved.\nThis traits specifies a way to rename the queried names …\nA simple <code>VarBuilder</code>, this is less generic than …\nA structure used to retrieve variables, these variables …\nThis returns true only if a tensor with the passed in name …\nThe device used by default.\nThe dtype used by default.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInitializes a <code>VarBuilder</code> using a custom backend.\nInitializes a <code>VarBuilder</code> using a custom backend.\nInitializes a <code>VarBuilder</code> from a binary buffer in the …\nInitializes a <code>VarBuilder</code> from a binary buffer in the …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> from a binary slice in the …\nInitializes a <code>VarBuilder</code> from a binary slice in the …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> using a <code>VarMap</code>. The requested …\nInitializes a <code>VarBuilder</code> using a <code>VarMap</code>. The requested …\nRetrieve a tensor with some target shape.\nRetrieve a tensor based on a target name and shape.\nRetrieve the tensor associated with the given name at the …\nRetrieve a tensor based on the name.\nRetrieve a tensor based on the name.\nRetrieve the tensor associated with the given name at the …\nRetrieve the tensor associated with the given name &amp; dtype …\nRetrieve the tensor associated with the given name at the …\nRetrieve the tensor associated with the given name &amp; dtype …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nShort alias for <code>push_prefix</code>.\nReturns the prefix of the <code>VarBuilder</code>.\nReturn a new <code>VarBuilder</code> adding <code>s</code> to the current prefix. …\nThis is applied to the name obtained by a name call and …\nGets a VarBuilder that applies some renaming function on …\nGets a VarBuilder that applies some renaming function on …\nReturns a new <code>VarBuilder</code> using the root path.\nSet the device of the VarBuilder.\nSet the dtype of the VarBuilder.\nReturns a new <code>VarBuilder</code> with the prefix set to <code>prefix</code>.\nClone the VarBuilder tweaking its dtype\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that uses zeros for any tensor.\nInitializes a <code>VarBuilder</code> that uses zeros for any tensor.\nA <code>VarMap</code> is a store that holds named variables. Variables …\nRetrieve all the variables currently stored in the map.\nReturns the argument unchanged.\nRetrieve or add a new variable.\nRetrieve or add a new variable.\nCalls <code>U::from(self)</code>.\nLoad some values from a safetensors file and modify the …\nCreate a new empty <code>VarMap</code>.\nSave the map in the safetensors format.\nSet some named variables to some values.\nSet a named variable to some value.")